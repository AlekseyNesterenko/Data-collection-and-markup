## Семинар 7. Selenium в Python
* Домашнее задание: 
1.	Выберите веб-сайт, который содержит информацию, представляющую интерес для извлечения данных. Это может быть 
новостной сайт, платформа для электронной коммерции или любой другой сайт, который позволяет осуществлять скрейпинг 
(убедитесь в соблюдении условий обслуживания сайта).
2.	Используя Selenium, напишите сценарий для автоматизации процесса перехода на нужную страницу сайта.
3.	Определите элементы HTML, содержащие информацию, которую вы хотите извлечь (например, заголовки статей, названия 
продуктов, цены и т.д.).
4.	Используйте BeautifulSoup для парсинга содержимого HTML и извлечения нужной информации из идентифицированных элементов.
5.	Обработайте любые ошибки или исключения, которые могут возникнуть в процессе скрейпинга.
6.	Протестируйте свой скрипт на различных сценариях, чтобы убедиться, что он точно извлекает нужные данные.
7.	Предоставьте ваш Python-скрипт вместе с кратким отчетом (не более 1 страницы), который включает следующее: 
URL сайта. Укажите URL сайта, который вы выбрали для анализа. Описание. Предоставьте краткое описание информации, 
которую вы хотели извлечь из сайта. Подход. Объясните подход, который вы использовали для навигации по сайту, 
определения соответствующих элементов и извлечения нужных данных. Трудности. Опишите все проблемы и препятствия, с 
которыми вы столкнулись в ходе реализации проекта, и как вы их преодолели. Результаты. Включите образец извлеченных 
данных в выбранном вами структурированном формате (например, CSV или JSON).  
*Примечание: Обязательно соблюдайте условия 
обслуживания сайта и избегайте чрезмерного скрейпинга, который может нарушить нормальную работу сайта.*

## Решение

В качестве цели для скрейпинга выберем сайт ЮНЕСКО [unesco.org](https://www.unesco.org/en/world-heritage/list)

Для поиска - все объекты всемироного наследия всех стран.

Результатом работы долен быть JSON файл со следующей информацией о конкретном архитектурном иил природным наследием:
- Название
- Страна местонахождения
- Координаты


Для реализации задачи сразу используем список из всех объектов, где они не разделены по страннам и идут в алфавитном порядке для удобства. 

Ввывод информации будет дозированным - поэтому нужно будет предусмотреть скроллинг
страницы вниз.

После получения страницы со всеми результатами - мы сможем получить все ссылки.

Оформим все ссылки в список и отправим каждую на парсинг каждой страницы книги, с помощью
BeautifulSoup. Тут мы получим интересующие нас данные. 

После этого - запишем всю найденную информацию в JSON файл.

Трудности вызвало нахождение подходящего сайта для скрапинга (сайты, которые интересовавли, имели ограничения по скрейпингу). Так же трудности возникли при скроллинге страницы и с сертификатом Chrome (пришлось скрыть оповещение, так как оно не влияет на работу, но выводит лишнюю информацию в терминал).


Решение находится в файле *[app.py](app.py)*

### Результат работы: 

Вывод программы:

    PS C:\Users\lesha\OneDrive\Рабочий стол\Обучение\Сбор и разметка данных\Lesson 7> python app.py

    DevTools listening on ws://127.0.0.1:59151/devtools/browser/ea27af98-8515-490c-979b-0e5c0130f611
    Created TensorFlow Lite XNNPACK delegate for CPU.


Файл с полученными данными: [unesco.json](unesco.json)

Часть JSON файла:

    [
    [
        "‘Uruq Bani Ma’arid",
        "Saudi Arabia",
        "N19 21 50 E45 35 54"
    ],
    [
        "18th-Century Royal Palace at Caserta with the Park, the Aqueduct of Vanvitelli, and the San Leucio Complex",
        "Italy",
        "N41 4 23.988 E14 19 35.004"
    ],
    [
        "Aachen Cathedral",
        "Germany",
        "N50 46 29.089 E6 5 2.112"
    ],
    [
        "Aapravasi Ghat",
        "Mauritius",
        "S20 9 31.1 E57 30 11.4"
    ],